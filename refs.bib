@misc{dettmers2023qloraefficientfinetuningquantized,
  title           = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author          = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke
                  Zettlemoyer},
  year            = 2023,
  eprint          = {2305.14314},
  archivePrefix   = {arXiv},
  primaryClass    = {cs.LG},
  url             = {https://arxiv.org/abs/2305.14314},
}

@misc{srinivasan2024comparativeanalysisdifferentefficient,
  title           = {Comparative Analysis of Different Efficient Fine Tuning
                  Methods of Large Language Models (LLMs) in Low-Resource
                  Setting},
  author          = {Krishna Prasad Varadarajan Srinivasan and Prasanth Gumpena
                  and Madhusudhana Yattapu and Vishal H. Brahmbhatt},
  year            = 2024,
  eprint          = {2405.13181},
  archivePrefix   = {arXiv},
  primaryClass    = {cs.CL},
  url             = {https://arxiv.org/abs/2405.13181},
}

@misc{hu2021loralowrankadaptationlarge,
  title           = {LoRA: Low-Rank Adaptation of Large Language Models},
  author          = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan
                  Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu
                  Chen},
  year            = 2021,
  eprint          = {2106.09685},
  archivePrefix   = {arXiv},
  primaryClass    = {cs.CL},
  url             = {https://arxiv.org/abs/2106.09685},
}

@misc{liu2023visualinstructiontuning,
  title           = {Visual Instruction Tuning},
  author          = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae
                  Lee},
  year            = 2023,
  eprint          = {2304.08485},
  archivePrefix   = {arXiv},
  primaryClass    = {cs.CV},
  url             = {https://arxiv.org/abs/2304.08485},
}

@misc{zhao2025redonerevealingdomainspecificllm,
  title           = {RedOne: Revealing Domain-specific LLM Post-Training in
                  Social Networking Services},
  author          = {Fei Zhao and Chonggang Lu and Yue Wang and Zheyong Xie and
                  Ziyan Liu and Haofu Qian and JianZhao Huang and Fangcheng Shi
                  and Zijie Meng and Hongcheng Guo and Mingqian He and Xinze Lyu
                  and Yiming Lu and Ziyang Xiang and Zheyu Ye and Chengqiang Lu
                  and Zhe Xu and Yi Wu and Yao Hu and Yan Gao and Jun Fan and
                  Xiaolong Jiang and Weiting Liu and Boyang Wang and Shaosheng
                  Cao},
  year            = 2025,
  eprint          = {2507.10605},
  archivePrefix   = {arXiv},
  primaryClass    = {cs.LG},
  url             = {https://arxiv.org/abs/2507.10605},
}

@software{unsloth,
  author          = {Daniel Han, Michael Han and Unsloth team},
  title           = {Unsloth},
  url             = {http://github.com/unslothai/unsloth},
  year            = 2023
}

@misc{mediumFinetuningBERT,
  author          = {Anthony Galtier},
  title           = {{F}ine-tuning {B}{E}{R}{T} for a regression task: is a
                  description enough to predict a propertyâ€™s list price? ---
                  medium.com},
  howpublished    =
                  {\url{https://medium.com/ilb-labs-publications/fine-tuning-bert-for-a-regression-task-is-a-description-enough-to-predict-a-propertys-list-price-cf97cd7cb98a}},
  year            = 2021,
}

@INPROCEEDINGS{9923496,
  author          = {Kakar, Surbhi and Dhaka, Deepali and Mehrotra, Monica},
  booktitle       = {2022 International Conference on Advanced Computer Science
                  and Information Systems (ICACSIS)},
  title           = {Clustered Bert Model for predicting Retweet Popularity},
  year            = 2022,
  pages           = {7-12},
  keywords        = {COVID-19;Radio frequency;Computational modeling;Supervised
                  learning;Predictive models;Prediction algorithms;Data
                  models;retweet prediction;bert;clustering;unsupervised machine
                  learning;supervised machine learning;retweet
                  popularity;retweet diffusion},
  doi             = {10.1109/ICACSIS56558.2022.9923496}
}
